{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](http://ocausal.imbv.net/wp-content/uploads/2017/02/banner-autocop-3.jpg)\n",
    "[AutoCop](http://ocausal.imbv.net/proyecto-autocop-es/), Proof of Concept of the  Observatorio de Contenidos Audiovisuales ([OCA](http://ocausal.imbv.net/proyecto-autocop-es/)), funded by the University of Salamanca Foundation [Plan TCUE 2015-2017 Fase 2]. \n",
    "Principal Investigator: Carlos Arcila Calderón. Researchers: Félix Ortega, Javier Amores, Sofía Trullenque, Miguel Vicente, Mateo Álvarez, Javier Ramírez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoCop to run in Spark in English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark application\n",
    "This application starts a streaming context connected to a kafka topic with a kafka consumer and predicts the score of tweets with MLlib algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries and start Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "import os\n",
    "import datetime\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark import SparkConf\n",
    "\n",
    "SUBMIT_ARGS = \"--packages org.apache.spark:spark-streaming-kafka-0-8-assembly_2.11:2.1.0,\\\n",
    "org.mongodb.spark:mongo-spark-connector_2.10:2.0.0 pyspark-shell\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = SUBMIT_ARGS\n",
    "#conf = SparkConf()\n",
    "conf = (SparkConf()\\\n",
    "        .set(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/twitter.tests\") \\\n",
    "        .set(\"spark.mongodb.output.uri\", \"mongodb://localhost:27017/twitter.tests\"))\n",
    "    \n",
    "sc = pyspark.SparkContext(appName=\"streaming_app\", conf=conf) \\\n",
    ".getOrCreate()\n",
    "spark = pyspark.sql.SparkSession(sc)\n",
    "\n",
    "from pyspark.streaming import StreamingContext\n",
    "ssc = StreamingContext(sc, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kafka configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the server configuration and the topic to read from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kafka_configuration_params = {\n",
    "    \"topic\": [\"BigData\"],\n",
    "    \"connectionstring\": \"localhost:9092\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "directKafkaStream = KafkaUtils.createDirectStream(\n",
    "    ssc, kafka_configuration_params[\"topic\"],\n",
    "    {\"metadata.broker.list\": kafka_configuration_params[\"connectionstring\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the stored algorithms, the algorithms have to have been trained before with the script provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import SVMModel, LogisticRegressionModel, NaiveBayesModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classif_LR_model = LogisticRegressionModel.load(sc, \"LR_model\")\n",
    "classif_SVM_model = SVMModel.load(sc, \"SVM_model\")\n",
    "classif_NB_model = NaiveBayesModel.load(sc, \"NB_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LR_model = classif_LR_model#.clearThreshold() only if saved without thresshold\n",
    "SVM_model = classif_SVM_model#.clearThreshold() only if saved without thresshold\n",
    "NB_model = classif_NB_model # NB already generates probabilities only if saved without thresshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Transform streaming rdds to match algorithms input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to classify the tweets, these have to pass through the same process the training instances passed, to transform each tweet into a Spark's Labeled Point instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.tokenize import word_tokenize\n",
    "allowed_word_types = [\"JJ\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the word list generated during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd_all_words = sc.textFile(\"all_words/part-00000\")\n",
    "rdd_broadcast_all_words = sc.broadcast(rdd_all_words.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert_tweet_to_instance(tweets):\n",
    "    \n",
    "    rdd_tweets = tweets.map( \\\n",
    "    lambda tweet: [word[0] for word in nltk.pos_tag(word_tokenize(tweet)) if word[1] in allowed_word_types])\n",
    "    \n",
    "    rdd_instances = rdd_tweets.map(lambda instance: find_features(instance))\n",
    "    \n",
    "    return rdd_instances\n",
    "\n",
    "def find_features(instance):\n",
    "    features = []\n",
    "    for word in rdd_broadcast_all_words.value:\n",
    "        if word in instance:\n",
    "            features.append(1)\n",
    "        else:\n",
    "             features.append(0)   \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Command to start streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get tweets from kafka stream and save the text to a RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd_input = directKafkaStream.map(lambda output: output[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the tweets to instances with the same transformation of the training process and user them to predict the label with the three models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classification = convert_tweet_to_instance(rdd_input).map(lambda instance: \\\n",
    "1 if (LR_model.predict(instance) + int(NB_model.predict(instance)) + SVM_model.predict(instance))>=1 else -1)\n",
    "\n",
    "classification_each = convert_tweet_to_instance(rdd_input).map(lambda instance: \\\n",
    "[LR_model.predict(instance), NB_model.predict(instance), SVM_model.predict(instance)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark streaming actions: printing each prediction with standard output and writting predictions and tweets to files (Spark creates a new folder with the results for each batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#classification_each.pprint()\n",
    "#classification.saveAsTextFiles(\"file:///Users/carlosarcila/Development/autocop_dis/notebooks/result\", \"txt\")\n",
    "#rdd_input.saveAsTextFiles(\"file:///Users/carlosarcila/Development/autocop_dis/notebooks/tweet\", \"txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use database to store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_to_db(rdd, collection):\n",
    "    df = rdd.zipWithUniqueId().toDF().withColumn('timestamp',lit(datetime.datetime.utcnow()))\\\n",
    "    .toDF('label','in_batch_id', 'timestamp')\n",
    "\n",
    "    df.write.format(\"com.mongodb.spark.sql.DefaultSource\").mode(\"append\").option(\"database\",\n",
    "\"twitter\").option(\"collection\",collection).save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classification.foreachRDD(lambda rdd: save_to_db(rdd,\"labels\"))\n",
    "rdd_input.foreachRDD(lambda rdd: save_to_db(rdd, \"tweets\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6016"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(NB_model.theta[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
