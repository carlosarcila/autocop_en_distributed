{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](http://ocausal.imbv.net/wp-content/uploads/2017/02/banner-autocop-3.jpg)\n",
    "[AutoCop](http://ocausal.imbv.net/proyecto-autocop-es/), Proof of Concept of the  Observatorio de Contenidos Audiovisuales ([OCA](http://ocausal.imbv.net/proyecto-autocop-es/)), funded by the University of Salamanca Foundation [Plan TCUE 2015-2017 Fase 2]. \n",
    "Principal Investigator: Carlos Arcila Calderón. Researchers: Félix Ortega, Javier Amores, Sofía Trullenque, Miguel Vicente, Mateo Álvarez, Javier Ramírez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoCop to run in Spark in English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter API to kafka producer\n",
    "\n",
    "This app connects to Twitter API and sends the stream to a kafka producer to make the data available from the kafka broker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is intended to use the power of spark to distribute the classification of the tweets from the twitter stream as they are published. To do so, we will use the Twitter API connected to a kafka server so that the information is available through a broker, separating the hashtags each one in a different kafka topic.\n",
    "\n",
    "As for the tweet classification, it is done by tokenizing each one and comparing it's words to the ones from the training set. \n",
    "To train the models, and also for the classification, the first step is to convert each tweet into an array of features, being the features a list of selected adjetives, gathered from the training set. As a result of the transformation, we will get an array of 0's and 1's, indicating the ausence or presence of each of the words in the list in the instance tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Kafka Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Kafka on local machine with single instance of Zookeeper and Kafka\n",
    "\n",
    "### Download kafka\n",
    "\n",
    "The first step is to download Apache Kafka, the version this script uses is Scala 2.11  - kafka_2.11-0.10.2.0.tgz (asc, md5), the download link is the following: http://mirror.nexcess.net/apache/kafka/0.10.2.0/kafka_2.11-0.10.2.0.tgz\n",
    "\n",
    "Once downloaded and unzipped the folder to a desired location, setup configuration for zookeeper and kafka services\n",
    "\n",
    "### Zookeeper and Kafka configuration\n",
    "\n",
    "The configuration files are stored in kafka_folder/config/\n",
    "    Zookeeper: zookeeper.properties\n",
    "    Kafka: server.properties\n",
    "    \n",
    "Here are some example files:\n",
    "\n",
    "zookeeper.properties\n",
    "server.properties\n",
    "\n",
    "### Start Zookeeper service\n",
    "\n",
    "To run the kafka server, a zookeeper instance is necessary, you can use one of yours or the one included in the kafka folder. To start the zookeeper: kafka_folder/bin/zookeeper-server-start.sh config/zookeeper.properties\n",
    "\n",
    "### Start Kafka Server\n",
    "\n",
    "Once Zookeeper is running, we can start kafka server with this command: kafka_folder/bin/kafka-server-start.sh config/server.properties\n",
    "\n",
    "For now we have Zookeeper and a Kafka server running, to start recieving and processing twitter streaming, we have to start a kafka producer from the twitter stream and then read the stream from the broker through spark streaming using the Kafka Stream Class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Kafka producer\n",
    "\n",
    "To start the Kafka producer, just execute the following cells.\n",
    "These cells use the tweepy library to connect to the Twitter API, to do so, credentials for the API must be provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from kafka import SimpleProducer, KafkaClient\n",
    "import tweepy\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class that extends tweepy stream listener that connects to the Twitter API and sends the data to a Kafka Producer with a specified topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "Some parameters have to be configured.\n",
    "\n",
    "First of all the Twitter API parameters, which require auth params as well as a hashtag, then we have to configure kafka parameters.\n",
    "Kafka will require the location of the kafka server, the upodate frequency of the producer (frequency to gather tweets from the API), and the topic to write on, this parameter is taken from the hashtag, and the kafka topic created for each hashtag will have the name of each one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter API parameters\n",
    "\n",
    "#### Auth parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "twitter_credentials = {\n",
    "    \"consumer_key\": \"bWzJx7DkIehLPBLsFuB0Q0HeG\",\n",
    "    \"consumer_secret\": \"wYk2PgEqDm0b9h5fHJTM4GGeIqyO9epWck7rHheLa615i2CCid\",\n",
    "    \"access_key\": \"13291482-SeGkyyTUTikUaEM8Q4vnJBHVnsCBR0cz3v6rhMAt1\",\n",
    "    \"access_secret\": \"ZGzaXe68bFwy7hT65Lbpi8WT5lh6cplRUU6FkbEx1IzLz\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashtag parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twitter_parameters = {\n",
    "    \"hashtag\": [\"#BigData\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kafka producer parameters\n",
    "These parameters controll the workflow of the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kafka_producer_parameters = {\n",
    "    \"batch_send_freq_t\": 1000,\n",
    "    \"batch_send_freq_n\": 10,\n",
    "    \"topic\": twitter_parameters[\"hashtag\"][0][1:],\n",
    "    \"connection_string\": \"localhost:9092\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TwitterStreamingListener\n",
    "\n",
    "The tweepy library uses a StreamListener Class to connect to the Twitter API. We will create a class extending from the previous that will send the twitter streaming to a Kafka producer and post each tweet on the specified topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TwitterStreamingListener(tweepy.StreamListener):\n",
    "\n",
    "    def __init__(self, api, kafka_producer_parameters):\n",
    "        self.api = api\n",
    "        self.kafka_producer_parameters = kafka_producer_parameters\n",
    "        super(tweepy.StreamListener, self).__init__()\n",
    "        client = KafkaClient(kafka_producer_parameters[\"connection_string\"])\n",
    "        self.producer = SimpleProducer(client, async = True,\n",
    "                          batch_send_every_n = kafka_producer_parameters[\"batch_send_freq_t\"],\n",
    "                          batch_send_every_t = kafka_producer_parameters[\"batch_send_freq_n\"])\n",
    "        \n",
    "    def on_status(self, status):\n",
    "        \"\"\" This method is called whenever new data arrives from live stream.\n",
    "        We asynchronously push this data to kafka queue\"\"\"\n",
    "        msg =  status.text.encode('utf-8')\n",
    "        try:\n",
    "            self.producer.send_messages(kafka_producer_parameters[\"topic\"], msg)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "        # Error in Kafka producer\n",
    "        print(status)\n",
    "        return True\n",
    "    \n",
    "    def on_timeout(self):\n",
    "        print(\"Timeout on twitter API\")\n",
    "        return True # Don't kill the stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Application\n",
    "Create the auth object and start application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#BigData']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_parameters[\"hashtag\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create Auth object\n",
    "auth = tweepy.OAuthHandler(twitter_credentials[\"consumer_key\"], twitter_credentials[\"consumer_secret\"])\n",
    "auth.set_access_token(twitter_credentials[\"access_key\"], twitter_credentials[\"access_secret\"])\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "# Create stream and bind the listener to it\n",
    "stream = tweepy.Stream(auth, listener = TwitterStreamingListener(api, kafka_producer_parameters))\n",
    "\n",
    "#Custom Filter rules pull all traffic for those filters in real time.\n",
    "stream.filter(track=twitter_parameters[\"hashtag\"])\n",
    "#stream.filter(track = twitter_parameters[\"hashtag\"], languages = ['es'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow the logs\n",
    "After launching the kafka application, the tweets can be seen introducing this command in the terminal, in the kafka folder: ./bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic BigData --from-beginning --zookeeper 192.168.0.11:2181 where BigData is the topic to be read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kafka consumer\n",
    "Once the previous infrastructure is running, we have a kafka producer connected to the twitter API. To analyze the tweets with Spark, just launch the twitter-kafka-consumer-spark-streaming.ipynb notebook (the models and the words have to have been generated before with the twitter-spark-model-training.ipynb notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
